\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage[utf8]{inputenc}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Computer Vision Lab 07 -PHOW }

\author{Juan Carlos Leon Alcazar\\
Universidad de los Andes\\
{\tt\small jc.leon@uniandes.edu.co}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
\\
\\
\\
{\tt\small }
}

\maketitle
%\thispagestyle{empty}


\section{Description of the datasets}
There are two main image databases used for this laboratory:

\begin{description}
\item[Caltech 101:] The caltech 101 dataset contains a total of 9145 images distributed in 101 categories. Each category contains between 40 to 800 color images, most of them are catalog images a contain a single instance of a single object. Most of the images are pictures of the object and a few are hand draw.  There is some variability in the resolution of images, but most of them have a resolution of around 250x300 \cite{Fei-Fei2007}.

\item[ImageNet:] The subset of the image net contains 996 object categories each with 100 instances for a total of 199200 images. All images are in format JPEG contain color information, the image size is standard on the dataset at 250x250. Unlike caltech 101 The categories from imagenet are created according to the  WordNet hierarchy.\cite{Deng2009}
\end{description}


\section{Recognition Method}

The base recognition method for the laboratory is the “pyramid histogram of visual words” also know as PHOW\cite{Bosch2007}. In this method, images are represented by means of an spatial pyramid as proposed proposed by Lazebnik\cite{Lazebnik2006}. Such pyramid  is built upon a regular grid of  N  partitions, where N increases(decreases) as the level of the pyramyd increases (decreases). This representation is thus an orderless collection of multi scale feature histograms for a single image.

The features calculated at each region of the grid are the SIFT\cite{Lowe1999} descriptors proposed by lowe. These features are scale and rotation invariant descriptors created from the gradient information in region of an image. The multi scale features are used to build a codebook or dictionary by using a clustering method (usually Means or Expectation Maximization). This codebook is then used to quantize the descriptor vector for every other image in the train set. Finally, the whole set of quantized vectors is used to train a classifier SVM,

For the classification step features are calculated and quantized on the test set under the same scheme described for the train set, Finally the trained SVM is used for classification.
\subsection{Changes over the base recognition method} \label{sssec:num1}
The code base for this laboratory is provided in http://www.vlfeat.org/applications/caltech-101-code.html  As a matlab scripts which uses the VLFeat Library\cite{Vedaldi2010} to implement the classification process described above.

Before directly using the classifier in the imagenet database, we first try to optimize the hyperparameters in the smaller (and easier caltech) 101 set. The main goal of this procedure is to avoid the hyper parameter search in the imagenet dataset as there is not enough time of complete such task at that scale.

In the base PHOW code we identify 7 Hyperparameters for different stages of the problem \footnote{The upper limits for the number of iterations in the clustering process and the depth of the KDTree and the image resize operation are also parameters but are considered of far less importance than the mentioned above}
\begin{itemize}
	\item Number of words for dictionary (any integer number larger than 0)
	\item Spatial partition in X  (any combination of N - $N > 0$ - integers larger than 1)
	\item Spatial partition in Y  (any combination of N - $N > 0$ - integers larger than 1)
	\item Svm bias multiplier (Any real number)
	\item Clustering algorithm  (Three different approximations to the EM algorithm are available)
	\item Max number of iterations for the clustering algorithm (Positive Integer)
	\item Type of kernel (Chi square, intersection  kernel, Jensen-Shannon kernel)
	\item SVM C (Real number)
	\item SVM solver (sdca, sgd and linear are available)
	\item Svm Max Number of Iterations (positive integer)
	\item Epsilon (goodness of fit) of svm. (Real number)
\end{itemize}


The initial number of possible hyper parameters combinations is not even bounded. To better approach this problem (given the time and computing power limitations), we approach the hyper parameter search by optimizing a single hyper parameter at the time, find the best possible accuracy for it, and then proceed to optimize a second hyperparameter. The full set of hyper parameter is not used, as we select a subset of those who could produce larger improvements in the classification process:

This is the order for the parameter optimization, we briefly describe the reason to select the parameter:

\begin{enumerate}
	\item Number of words (Changes the number of visual words in the codebook, which are then used to represent every word
in the training and test set. This parameter essentially tunes the initial data representation)
	\item Spatial partitions (Like the number of words,  this parameters changes size of the visual words in the codebook,Again this parameter tunes the initial data  representation)
	\item SVM C (Controls the loss function of the svm, and thus the global optimization process, by assigning a penalty to the misclassified elements)
	\item Kernel (specifies a N-dimensional space where the linear separation of the data is performed, a space where the data is approximately linearly separable will bring the best results)
	\item Solver (The algorithm used to linearly separate the data of the SVM)
	\item Clustering algorithm (create the codebook from the initial representation of the training images, like the number of words has a great influence in the initial data representation)
\end{enumerate}
The initial parameter tuning was performed  with a subset of  from caltech (30 images - 15 test, 15 train-) per class with 30 classes.

The best set of hyper parameters is:
\begin{itemize}
	\item Number of words: 1000
	\item Spatial partitions: [2,3,4]
	\item SVM C: 50
	\item Kernel:  Intersection  Kernel
	\item Solver: SDCA
	\item Clustering algorithm: ANN
\end{itemize}

The performance of this set of parameters over  the subset of 30 classes is 80.67\%, The performance over the whole set of 100 classes and 9145 Images degrades significantly to is 68\%. This figure is only slightly better than the initial configuration at 67.50\%.

\subsection{Results on Imagenet}

Producing a result for the whole dataset is a challenging task. It is not possible to train with the whole 99600 set of training images, it was empirically established that the RAM memory on the server (100GB) can handle the whole classification process with up to 10000 images.  This means that either a subset of the images must be selected or a subset of the categories must be selected, we use both approaches

\begin{itemize}
\item Select all 996 categories and 10 images for each class (10000 images total)
\item Select only 100 categories (random) and use all 100 images for each class (10000 images total)
\item A third 'hybrid' strategy  with 310 categories and 31 images for each class (9610 images total)
\end{itemize}

Results are summarized in table \ref{table:tableFA}
\begin{table}[t]
\centering
\begin{tabular}{c | c }
Sampling & Accuracy    \\
\hline	
996 Categories, 10 images per class & 12.80  \\
100 Categories, 100 images per class & 19.84 \\
310 Categories, 31 images per class & 14.86 \\

\end{tabular}
\caption{Acurracy over the full image net dataset}
\label{table:tableFA}
\end{table}


\subsection{Sensitivity to Parameters}
The initial configuration of hyper parameters (see section \ref{sssec:num1}) is used as a base configuration to analyse the sensitivity of the method to changes in a subset of hyperparameters in the imagenet dataset, The set of hyper parameters explored is:

\begin{description}
\item[Number of Categories] set of the parameters to explore: 50,100,200,500,1000
\item[Size of training set] set of the parameters to explore: 10,20,50,100
\item[Number of spatial partitions] set of the parameters to explore: [2 4]  [2 3 4]  [2 3 4 5]  [2 4 8]  [2 4 16] .
\end{description}

results are shown in tables \ref{table:tableCategories}, \ref{table:tableImages} and \ref{table:tableClases}.

\begin{table}[t]
\centering
\begin{tabular}{c | c }
Clases & Accuracy    \\
\hline	
10 & 44  \\
50 & 21.20 \\
100 & 13 \\
200 & 13 \\
500 & 10.08 \\

\end{tabular}
\caption{Accuracy according to the number of classes used for training and test. No test with the complete dataset are performed as the kaget test (500) classes already consumes over 100GB of RAM memory.
}
\label{table:tableClases}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{c | c }
Number of Images & Accuracy    \\
\hline	
5 & 25.10  \\
10 & 36.00 \\
20 & 45.00 \\
50 & 52.20 \\
100 & 56.00 \\

\end{tabular}
\caption{Accuracy according to the number of images per class used for training and test.
}
\label{table:tableImages}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{l | c }
Spatial Partition & Accuracy    \\
\hline	
2 4 & 25.10  \\
2 3 4  & 36.00 \\
2 3 4 5 & 45.00 \\
2 4 8 & 52.20 \\
2 4 8 16 & 56.00 \\

\end{tabular}
\caption{Accuracy according to the spatial partition (same partition sued for both axes) per image used for training and test.
}
\label{table:tableCategories}
\end{table}

\section{Results Analysis \& limitations}

Clearly the method is not good enough on the problem of classification on the imagenet dataset, the overall accuracy is under 20\% for the larger possible experiments  that can be handled with the available hardware.

Overall the method is insensitive to the spatial partitions and the initially suggested pyramyd of 2 and 4 partitions has the same accuracy of deeper pyramys. This suggest that the relevant information can be captured at the first two scale, and the additional data of other scales is likely noisy data discarded by the classifier or the clustering process.

It is clear that the classification improves the larger the training set is. This could be attributed to the large amount of different samples from each class which contribute to build a better model of the object (provided the scales and number of classes remains stable).

It is interesting to notice that the smaller the amount of classes in the classification process the better the results. This suggest that either there is not enough capacity in the model to capture the complete variability of a large number of classes, or the base descriptor (Sift) is not capable of capturing the relevant information when the classification problem grows large.

Finally it is also worth noticing that the procedures requires a very large amount of computer resources, the subset used for the laboratory is an order of magnitude smaller than the complete imagenet dataset. Applying this method to the full dataset will probably require require a very large computer or cluster with Terabytes of RAM memory available. Even with today's hardware such requirements are not easy to meet.

\subsection{Improvemets}
A first approach to improve the results would be to perform a proper search over the full set of identified hyper-parameters, it is likely that the proposed approach could lead to a local minima.

Yet this process would not address the 2 most clear limitations found for this approach: Model capacity Base descriptors limitations

To address the first issue it would be necessary to change the kernel for the svm, this process is, very difficult as there is not an straightforward method to optimize the representation space, rather this process is mostly (to the authors best knowledge) a trial and error approach. Given the large size of the problem optimizing the kernel  could be a very long task, whose results are not granted. Probably the best improvements would be to change the classifier for one that can better handle the large variability in the dataset, it is possible that a very large random forest could perform better in this scenario

Regarding the second limitation, the SIFT descriptors works at the local level, thus, ignoring regional and global patterns in the image that might be critical for the process of classification. A better approach might also use regional information (like the deformable part models) or global information as the second order statistics.

Overall, It is hard to address any of the problems only with modifications to the parameters of the model.

{\small
\bibliographystyle{ieee}
\bibliography{bibliography}
}


\end{document}

